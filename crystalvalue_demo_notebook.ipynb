{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "A9YRrFYNY37s",
      "metadata": {
        "id": "A9YRrFYNY37s"
      },
      "source": [
        "# DISCLAIMER\n",
        "Copyright 2021 Google LLC. \n",
        "\n",
        "*This solution, including any related sample code or data, is made available on an “as is,” “as available,” and “with all faults” basis, solely for illustrative purposes, and without warranty or representation of any kind. This solution is experimental, unsupported and provided solely for your convenience. Your use of it is subject to your agreements with Google, as applicable, and may constitute a beta feature as defined under those agreements. To the extent that you make any data available to Google in connection with your use of the solution, you represent and warrant that you have all necessary and appropriate rights, consents and permissions to permit Google to use and process that data. By using any portion of this solution, you acknowledge, assume and accept all risks, known and unknown, associated with its usage, including with respect to your deployment of any portion of this solution in your systems, or usage in connection with your business, if at all.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iTsKaxLPY37t",
      "metadata": {
        "id": "iTsKaxLPY37t"
      },
      "source": [
        "# Crystalvalue Demo: Predictive Customer LifeTime Value for a Retail Store\n",
        "\n",
        "Crystalvalue is a best practice comprehensive solution for running end-to-end LTV solutions leveraging Google Cloud Vertex AI.\n",
        "\n",
        "This demo runs the Crystalvalue python library in a notebook, from feature engineering to scheduling predictions. This notebook uses the Online Retail II data set from Kaggle which contains transactions for a UK retail store between 01/12/2009 and 09/12/2011. More details on this dataset can be found here https://www.kaggle.com/mashlyn/online-retail-ii-uci.\n",
        "\n",
        "This notebook assumes that it is being run from within a [Google Cloud Platform AI Notebook](https://console.cloud.google.com/vertex-ai/notebooks/list/instances) with a Compute Engine default service account (the default setting when an AI Notebook is created) and with a standard Python 3 environment.\n",
        "\n",
        "If you would like to share feedback about Crystalvalue, please email crystalvalue@google.com."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1_M6mv5_Al18",
      "metadata": {
        "id": "1_M6mv5_Al18"
      },
      "source": [
        "# Clone the Crystalvalue codebase"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hnySWWD2DbJ8",
      "metadata": {
        "id": "hnySWWD2DbJ8"
      },
      "source": [
        "If you have not cloned the Crystalvalue codebase yet, open up a terminal and execute the following commands.\n",
        "\n",
        "To clone the code you can authenticate by following these [steps](https://g3doc.corp.google.com/company/teams/gtech/ads/das/cse/faq/tools/professional-services-googlesource-com.md#working-from-gcp) as a Googler. To grant customer access to the codebase follow these [steps](https://g3doc.corp.google.com/company/teams/gtech/ads/das/cse/faq/tools/professional-services-googlesource-com.md#external-users)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6QSkrxSFsCB",
      "metadata": {
        "id": "a6QSkrxSFsCB"
      },
      "source": [
        "```git clone https://professional-services.googlesource.com/solutions/crystalvalue```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "giQW6SdYFWSu",
      "metadata": {
        "id": "giQW6SdYFWSu"
      },
      "source": [
        "Copy the notebook into your current working directory and run it there (in the parent directory of the crystalvalue directory)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lkP7Rx3xFwAK",
      "metadata": {
        "id": "lkP7Rx3xFwAK"
      },
      "source": [
        "```cp ./crystalvalue/crystalvalue_demo_notebook.ipynb .```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WJajLx_EtUKR",
      "metadata": {
        "id": "WJajLx_EtUKR"
      },
      "source": [
        "# Set up - Downloading the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iojPU3e4tW5z",
      "metadata": {
        "id": "iojPU3e4tW5z"
      },
      "source": [
        "In order to use the Kaggle’s public API, you must first authenticate using an API token. You can do this by visiting your Kaggle account and click 'Create New API Token' (See https://www.kaggle.com/docs/api). This will download an API token (called kaggle.json). Put this file in your working directory and run the following commands from your AI Notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "duYMfWZelG0f",
      "metadata": {
        "id": "duYMfWZelG0f"
      },
      "outputs": [],
      "source": [
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6EXbt-qetckB",
      "metadata": {
        "id": "6EXbt-qetckB"
      },
      "source": [
        "Kaggle requires the json to be in a specific folder called 'kaggle'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d-0X-BPrtcIJ",
      "metadata": {
        "id": "d-0X-BPrtcIJ"
      },
      "outputs": [],
      "source": [
        "!mkdir ~/.kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZyIa1VNTta-n",
      "metadata": {
        "id": "ZyIa1VNTta-n"
      },
      "outputs": [],
      "source": [
        "!cp kaggle.json ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qessvI1tthFT",
      "metadata": {
        "id": "qessvI1tthFT"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download -d mashlyn/online-retail-ii-uci"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cZPXzkP0tfoi",
      "metadata": {
        "id": "cZPXzkP0tfoi"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get install unzip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FQoIQXJBtjcZ",
      "metadata": {
        "id": "FQoIQXJBtjcZ"
      },
      "outputs": [],
      "source": [
        "!unzip online-retail-ii-uci.zip -d data/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sDA4zRGytlAx",
      "metadata": {
        "id": "sDA4zRGytlAx"
      },
      "source": [
        "This creates a `online_retail_II.csv` in `/data` which we will import into BigQuery in the next steps."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1O4OhKV5Y37u",
      "metadata": {
        "id": "1O4OhKV5Y37u"
      },
      "source": [
        "# Installing dependencies and initializing Crystalvalue"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XMYviqrPHbKD",
      "metadata": {
        "id": "XMYviqrPHbKD"
      },
      "source": [
        "First create a dataset in [Bigquery](https://console.cloud.google.com/bigquery) that will be used for this analysis if you don't already have one. The dataset location should be in a [location that Vertex AI services are available](https://cloud.google.com/vertex-ai/docs/general/locations#available_regions). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "m6gkj-aXY37u",
      "metadata": {
        "id": "m6gkj-aXY37u"
      },
      "outputs": [],
      "source": [
        "%pip install -q -r './crystalvalue/requirements.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mKWzgxQ1Y37v",
      "metadata": {
        "id": "mKWzgxQ1Y37v"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "from crystalvalue import crystalvalue\n",
        "from google.cloud import bigquery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TnNNkQNvY37v",
      "metadata": {
        "id": "TnNNkQNvY37v"
      },
      "outputs": [],
      "source": [
        "# Create BigQuery client for cloud authentication.\n",
        "bigquery_client = bigquery.Client()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ySWcHL7DcKrd",
      "metadata": {
        "id": "ySWcHL7DcKrd"
      },
      "outputs": [],
      "source": [
        "# Read the data and rename the columns to be BiqQuery friendly (no spaces).\n",
        "data = pd.read_csv('./data/online_retail_II.csv')\n",
        "data.columns = data.columns.str.replace(' ', '')\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0qlNEnCjcL9f",
      "metadata": {
        "id": "0qlNEnCjcL9f"
      },
      "outputs": [],
      "source": [
        "# Load the data to Bigquery.\n",
        "DATASET_ID = 'your_dataset_name'  # This is the name of the dataset that you created. \n",
        "TABLE_NAME = 'online_retail_data'  # This is what we will call the table that will be created in the dataset.\n",
        "LOCATION = 'europe-west4'  # This is the location of your dataset in Bigquery. Here we use 'europe-west4`.\n",
        "\n",
        "bigquery_job = bigquery_client.load_table_from_dataframe(\n",
        "      dataframe=data,\n",
        "      destination=f'{bigquery_client.project}.{DATASET_ID}.{TABLE_NAME}',\n",
        "      location=LOCATION).result()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gCVphBgXY37w",
      "metadata": {
        "id": "gCVphBgXY37w"
      },
      "outputs": [],
      "source": [
        "# Initiate the CrystalValue class with the relevant parameters.\n",
        "pipeline = crystalvalue.CrystalValue(\n",
        "  bigquery_client=bigquery_client,\n",
        "  dataset_id=DATASET_ID,\n",
        "  customer_id_column='CustomerID',\n",
        "  date_column='InvoiceDate',\n",
        "  value_column='Price',  #  Column to use for LTV calculation.\n",
        "  days_lookback=90,  #  How many days in the past to use for feature engineering.\n",
        "  days_lookahead=365,  #  How many days in the future to use for value prediction.\n",
        "  ignore_columns=['Invoice'],  #  A list of columns in your input table to ignore.\n",
        "  location=LOCATION\n",
        ")  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FHC77gHNcRDP",
      "metadata": {
        "id": "FHC77gHNcRDP"
      },
      "source": [
        "# Data Checks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xIfjG7zNcVtx",
      "metadata": {
        "id": "xIfjG7zNcVtx"
      },
      "source": [
        "CrystalValue will run some checks on your data to check if the data is suitable for LTV modelling and raise errors if not. This will also output a new BigQuery table in your dataset called `crystalvalue_data_statistics` with key information such as the number of customers, transactions and analysis time period. This information can be used to check for outliers or anomalies (e.g. negative prices). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CTiZxBvrcSeV",
      "metadata": {
        "id": "CTiZxBvrcSeV"
      },
      "outputs": [],
      "source": [
        "summary_statistics = pipeline.run_data_checks(\n",
        "    transaction_table_name=TABLE_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "emCUVYhLFnla",
      "metadata": {
        "id": "emCUVYhLFnla"
      },
      "source": [
        "If a custom data cleaning routine has to be implemented use the `.run_query()` method. The example below removes transactions with negative prices. This method could also be used to run custom feature engineering scripts instead of the automated `.feature_engineering()` method in the next step. This data cleaning routine can be scheduled as part of the pipeline that we will define later (for model training and prediction)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "q_hFsfRGFjxG",
      "metadata": {
        "id": "q_hFsfRGFjxG"
      },
      "outputs": [],
      "source": [
        "# Run a quick data cleaning routine that removes transactions with negative prices\n",
        "query = f\"\"\"\n",
        "SELECT *\n",
        "FROM {bigquery_client.project}.{DATASET_ID}.{TABLE_NAME}\n",
        "WHERE Price \u003e 0\n",
        "\"\"\"\n",
        "\n",
        "pipeline.run_query(\n",
        "    query_sql=query,\n",
        "    destination_table_name=TABLE_NAME,\n",
        "    location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5Bcu6lMlY37w",
      "metadata": {
        "id": "5Bcu6lMlY37w"
      },
      "source": [
        "# Feature Engineering\n",
        "\n",
        "Crystalvalue takes transaction or browsing level dataset and creates a machine learning-ready dataset that can be ingested by AutoML. Data types are automatically inferred from the BigQuery schema unless the features are provided using the `feature_types` parameter in the `.feature_engineer()` method. Data transformations are applied automatically depending on the data type. The data crunching happens in BigQuery and the executed script can be optionally written to your directory. The features will be created in a BigQuery table called `crystalvalue_train_data` by default."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yK5Lfb1SY37w",
      "metadata": {
        "id": "yK5Lfb1SY37w"
      },
      "outputs": [],
      "source": [
        "crystalvalue_train_data = pipeline.feature_engineer(\n",
        "  transaction_table_name=TABLE_NAME,\n",
        "  write_executed_query_file='crystalvalue/executed_train_query.sql'  # (Optional) File path to write the executed SQL query.\n",
        ")  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xQ0yaFbBY37x",
      "metadata": {
        "id": "xQ0yaFbBY37x"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3JCdyprSY37x",
      "metadata": {
        "id": "3JCdyprSY37x"
      },
      "source": [
        "Crystalvalue leverages [Vertex AI (Tabular) AutoML](https://cloud.google.com/vertex-ai/docs/training/automl-api) which requires a\n",
        "[Vertex AI Dataset](https://cloud.google.com/vertex-ai/docs/datasets/create-dataset-api) as an input. CrystalValue automatically creates a Vertex AI Dataset from your input table as part of the training step of the pipeline. The training process typically takes about 2 or more hours to run. The Vertex AI Dataset will have a display name `crystalvalue_dataset`. The model will have a display name `crystalvalue_model` but it will also receive a model ID (so even if you train multiple models they will not be overwritten and can be identified using these IDs). By default CrystalValue chooses the following parameters:\n",
        "*  Predefined split with random 15% of users as test, 15% in validation and 70% in\n",
        "training.\n",
        "*  Optimization objective as Minimize root-mean-squared error (RMSE). This is recommended but can be modified to [MAE or RMSLE](https://cloud.google.com/automl-tables/docs/train#opt-obj).\n",
        "*  1 node hour of training (1000 milli node hours), which we recommend starting with. [Modify this in line with the number of rows](https://cloud.google.com/automl-tables/docs/train#training_a_model) in the dataset when you are ready for productionising. See information here about [pricing](https://cloud.google.com/automl-tables/pricing).\n",
        "\n",
        "In this example we keep all the default settings so training the model is as simple as calling pipeline.train().\n",
        "\n",
        "In order to make fast predictions later, you can deploy the model using the `.deploy_model()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CSK1FtHVY37y",
      "metadata": {
        "id": "CSK1FtHVY37y"
      },
      "outputs": [],
      "source": [
        "# Creates AI Platform Dataset and trains AutoML model.\n",
        "model_object = pipeline.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dxA48EdTcs3c",
      "metadata": {
        "id": "dxA48EdTcs3c"
      },
      "outputs": [],
      "source": [
        "model_object = pipeline.deploy_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Dok2Wu6UY370",
      "metadata": {
        "id": "Dok2Wu6UY370"
      },
      "source": [
        "# Model Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wsJdx-MvY370",
      "metadata": {
        "id": "wsJdx-MvY370"
      },
      "source": [
        "To evaluate model, we use the following criteria:\n",
        "\n",
        "* The spearman correlation, a measure of how well the model **ranked** the Liftetime value of customers in the test set. This is measured between -1 (worse) and 1 (better).\n",
        "* The normalised Gini coefficient, another measure of how well the model **ranked** the Lifetime value of customers in the test set compared to random ranking. This is measured between 0 (worse) and 1 (better). \n",
        "* The normalised Mean Average Error (MAE%). This is a measure of the **error** of the model's predictions for Lifetime value in the test set. \n",
        "* top_x_percent_predicted_customer_value_share: The proportion of value (i.e. total profit or revenue) in the test set that is accounted for by the top x% model-predicted customers. \n",
        "\n",
        "These outputs are sent to a BigQuery table (by default called `crystalvalue_evaluation`). Subsequent model evaluations append model performance evaluation metrics to this table to allow for comparison across models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TmTV4LayjXo9",
      "metadata": {
        "id": "TmTV4LayjXo9"
      },
      "outputs": [],
      "source": [
        "metrics = pipeline.evaluate_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oJ6ZFLwrY37y",
      "metadata": {
        "id": "oJ6ZFLwrY37y"
      },
      "source": [
        "# Generating predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jEzDVyiMY37z",
      "metadata": {
        "id": "jEzDVyiMY37z"
      },
      "source": [
        "Once model training is done, you can generate predictions. Features need to be engineered (the exact same as were used for model training) before prediction. This is done using the `.feature_engineer()` method by setting the parameter `query_type='predict_query'`. The features will be created in a BigQuery table called crystalvalue_predict_data by default. The model will make predictions for all customers in the provided input table that have any activity during the lookback window. The pLTV predictions will be for the period starting from the last date in the input table (not today's date).  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PvryAnPFiuWG",
      "metadata": {
        "id": "PvryAnPFiuWG"
      },
      "source": [
        "Once you start the training, you can view your model training progress here:  \n",
        "https://console.cloud.google.com/vertex-ai/training/training-pipelines  \n",
        "Once the training is finished, check out your Dataset (with statistics and distributions) and Model (with feature importance) in the UI:  \n",
        " https://console.cloud.google.com/vertex-ai/datasets   \n",
        " https://console.cloud.google.com/vertex-ai/models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Z2sOiMID4X2-",
      "metadata": {
        "id": "Z2sOiMID4X2-"
      },
      "outputs": [],
      "source": [
        "crystalvalue_predict_data = pipeline.feature_engineer(\n",
        "    transaction_table_name=TABLE_NAME,  # An existing bigquery table in your dataset id containing the data to predict with.\n",
        "    query_type='predict_query')\n",
        "\n",
        "\n",
        "predictions = pipeline.predict(\n",
        "    input_table=crystalvalue_predict_data,\n",
        "    destination_table='predictions'  # The bigquery table to append predictions to. It will be created if it does not exist yet.\n",
        "    )  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bDF0AE95vvuv",
      "metadata": {
        "id": "bDF0AE95vvuv"
      },
      "source": [
        "# Scheduling predictions to occur every day"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uWrKLbrEv0xo",
      "metadata": {
        "id": "uWrKLbrEv0xo"
      },
      "source": [
        "Crystalvalue uses [Vertex Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines/introduction) to schedule and monitor ML predictions and retraining in a serverless manner. The example below demonstrates how to set up the model to automatically create predictions using new input data from the source BigQuery table every day at 1am. The frequency and timing of the schedule can be altered using the chron schedule below. Once this pipeline is set up, you can view it [here](https://console.cloud.google.com/vertex-ai/pipelines). If you want a tutorial on how to set up Vertex Pipelines [this guide](https://cloud.google.com/vertex-ai/docs/pipelines/build-pipeline) or see this [example notebook](https://github.com/GoogleCloudPlatform/ai-platform-samples/blob/master/ai-platform-unified/notebooks/official/pipelines/pipelines_intro_kfp.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZECtoCAyxbxX",
      "metadata": {
        "id": "ZECtoCAyxbxX"
      },
      "source": [
        "In order to use Vertex AI pipeline, we need a cloud storage bucket. Use the code below to create a cloud storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1zuWtrdwxjKX",
      "metadata": {
        "id": "1zuWtrdwxjKX"
      },
      "outputs": [],
      "source": [
        "BUCKET_NAME = f\"gs://{bigquery_client.project}-crystalvalue\" \n",
        "PIPELINE_ROOT = f\"{BUCKET_NAME}/pipeline_root\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZasQLxt4xmxL",
      "metadata": {
        "id": "ZasQLxt4xmxL"
      },
      "outputs": [],
      "source": [
        "!gsutil mb -l $LOCATION $BUCKET_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pj3GIIJr5TqC",
      "metadata": {
        "id": "pj3GIIJr5TqC"
      },
      "source": [
        "In order to use Vertex AI pipelines with Crystalvalue we also need to create a docker container which will be stored in Google Cloud Container Registry. The following code builds a docker container and pushes it to your [GCP Container Registry](https://cloud.google.com/container-registry).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xCEA0QZo5ale",
      "metadata": {
        "id": "xCEA0QZo5ale"
      },
      "outputs": [],
      "source": [
        "!docker build -t crystalvalue .\n",
        "!docker tag crystalvalue gcr.io/$PROJECT/crystalvalue\n",
        "!docker push gcr.io/$PROJECT/crystalvalue"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JlfrRkrF_jhR",
      "metadata": {
        "id": "JlfrRkrF_jhR"
      },
      "source": [
        "The Kubeflow components contains self-contained functions whichimport libraries and reinitiate objects. Read about [Kubeflow components](https://www.kubeflow.org/docs/components/pipelines/sdk/component-development/).  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jT1zxEqGwG17",
      "metadata": {
        "id": "jT1zxEqGwG17"
      },
      "outputs": [],
      "source": [
        "from kfp import dsl\n",
        "from kfp.v2 import compiler\n",
        "from kfp.v2.dsl import component\n",
        "from kfp.v2.google.client import AIPlatformClient\n",
        "\n",
        "@component(base_image=f\"gcr.io/{bigquery_client.project}/crystalvalue:latest\")\n",
        "def crystalvalue_data_checks():  \n",
        "  from crystalvalue import crystalvalue\n",
        "  from google.cloud import bigquery\n",
        "  bigquery_client = bigquery.Client(project=$PROJECT)  # Add your project\n",
        "  dataset_id = $DATASET  # Add your dataset \n",
        "  table_name = 'online_retail_data'  \n",
        "  location = 'europe-west1'\n",
        "  pipeline = crystalvalue.CrystalValue(\n",
        "    bigquery_client=bigquery_client,\n",
        "    dataset_id=dataset_id, \n",
        "    customer_id_column='CustomerID',\n",
        "    date_column='InvoiceDate',\n",
        "    value_column='Price',  \n",
        "    days_lookback=90,  \n",
        "    days_lookahead=365,  \n",
        "    location=location,\n",
        "    ignore_columns=['Invoice'])  \n",
        "  summary_statistics = pipeline.run_data_checks(\n",
        "    transaction_table_name=table_name)\n",
        "  \n",
        "\n",
        "@component(base_image=f\"gcr.io/{bigquery_client.project}/crystalvalue:latest\")\n",
        "def feature_engineering():  \n",
        "  from crystalvalue import crystalvalue\n",
        "  from google.cloud import bigquery\n",
        "  bigquery_client = bigquery.Client(project=$PROJECT)  # Add your project\n",
        "  dataset_id = $DATASET  # Add your dataset \n",
        "  table_name = 'online_retail_data'  \n",
        "  location = 'europe-west1'\n",
        "  pipeline = crystalvalue.CrystalValue(\n",
        "    bigquery_client=bigquery_client,\n",
        "    dataset_id=dataset_id, \n",
        "    customer_id_column='CustomerID',\n",
        "    date_column='InvoiceDate',\n",
        "    value_column='Price',  \n",
        "    days_lookback=90,  \n",
        "    days_lookahead=365,  \n",
        "    location=location,\n",
        "    ignore_columns=['Invoice'])  \n",
        "  data = pipeline.feature_engineer(\n",
        "    transaction_table_name=table_name)\n",
        "  \n",
        "  \n",
        "  \n",
        "@component(base_image=f\"gcr.io/{bigquery_client.project}/crystalvalue:latest\")\n",
        "def predict():  \n",
        "  from crystalvalue import crystalvalue\n",
        "  from google.cloud import bigquery\n",
        "  bigquery_client = bigquery.Client(project=$PROJECT)  # Add your project\n",
        "  dataset_id = $DATASET  # Add your dataset \n",
        "  table_name = 'online_retail_data'  \n",
        "  location = 'europe-west1'\n",
        "  pipeline = crystalvalue.CrystalValue(\n",
        "    bigquery_client=bigquery_client,\n",
        "    dataset_id=dataset_id, \n",
        "    customer_id_column='CustomerID',\n",
        "    date_column='InvoiceDate',\n",
        "    value_column='Price',  \n",
        "    days_lookback=90,  \n",
        "    days_lookahead=365,\n",
        "    ignore_columns=['Invoice'],  \n",
        "    location=location,\n",
        "    model_id=$MODEL_ID  # Add your model ID here e.g. '45252345252624534'\n",
        "    )\n",
        "  pipeline.batch_predict(\n",
        "    input_table_name='predict_features_data',  \n",
        "    destination_table='predictions')\n",
        "\n",
        "\n",
        "@dsl.pipeline(\n",
        "    name=\"crystalvaluepipeline\",\n",
        "    description=\"Runs predictions for crystalvalue\",\n",
        "    pipeline_root=PIPELINE_ROOT,\n",
        ")\n",
        "def crystalvalue_pipeline():\n",
        "    summary = crystalvalue_data_checks()\n",
        "    data = feature_engineering()\n",
        "    result = predict()\n",
        "    \n",
        "compiler.Compiler().compile(\n",
        "  pipeline_func=crystalvalue_pipeline,\n",
        "  package_path=\"crystalvaluepipeline.json\"\n",
        ")\n",
        "\n",
        "# Choose a region compatible with Vertex Pipelines. \n",
        "# This doesn't have to be the same as your data location.\n",
        "api_client = AIPlatformClient(\n",
        "    project_id=bigquery_client.project,\n",
        "    region='europe-west4',  \n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6jjXC_Cm8_bB",
      "metadata": {
        "id": "6jjXC_Cm8_bB"
      },
      "outputs": [],
      "source": [
        "# Optional: Check if you pipeline runs\n",
        "# api_client.create_run_from_job_spec(job_spec_path=\"crystalvaluepipeline.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NvdMHFG56t8o",
      "metadata": {
        "id": "NvdMHFG56t8o"
      },
      "outputs": [],
      "source": [
        "# Create the scheduled pipeline.\n",
        "# Adjust time zone and cron schedule as necessary.\n",
        "response = api_client.create_schedule_from_job_spec(\n",
        "    job_spec_path=\"crystalvaluepipeline.json\",\n",
        "    schedule=\"0 1 * * *\",\n",
        "    time_zone=\"America/Los_Angeles\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sxeSm7BxA5so",
      "metadata": {
        "id": "sxeSm7BxA5so"
      },
      "source": [
        "You can view your running and scheduled pipelines at:\n",
        "https://console.cloud.google.com/vertex-ai/pipelines"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "last_runtime": {
        "build_target": "//corp/gtech/ads/infrastructure/colab_utils/ds_runtime:ds_colab",
        "kind": "private"
      },
      "name": "crystalvalue_demo_notebook.ipynb",
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "/piper/depot/google3/third_party/professional_services/solutions/crystalvalue/crystalvalue_demo_notebook.ipynb?workspaceId=sumedhamenon:CrystalValue::citc",
          "timestamp": 1627656627042
        },
        {
          "file_id": "/piper/depot/google3/third_party/professional_services/solutions/crystalvalue/crystalvalue_demo_notebook.ipynb?workspaceId=sumedhamenon:CrystalValue::citc",
          "timestamp": 1627656460877
        },
        {
          "file_id": "/piper/depot/google3/experimental/gtech_prem_data_science/projects/trac/treatwell/Crystal_Value_Demo_Notebook.ipynb?workspaceId=sumedhamenon:CrystalValue::citc",
          "timestamp": 1627648785641
        },
        {
          "file_id": "1JGQRDc1_luQsaMxx9ZZavddHBrgO7Xst",
          "timestamp": 1627648471137
        }
      ]
    },
    "environment": {
      "name": "common-cpu.mnightly-2021-01-05-debian-10-test",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/base-cpu:mnightly-2021-01-05-debian-10-test"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
